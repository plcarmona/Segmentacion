{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-pcarmona@alges.cl/Model/Segmentacion\n"
     ]
    }
   ],
   "source": [
    "cd \"Model/Segmentacion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.ipynb         unetmIoU-0.211.pt  unetmIoU-0.285.pt  unetmIoU-0.293.pt\n",
      "data.py            unetmIoU-0.214.pt  unetmIoU-0.286.pt  unetmIoU-0.296.pt\n",
      "optuna.db          unetmIoU-0.215.pt  unetmIoU-0.287.pt  unetmIoU-0.734.pt\n",
      "\u001b[0m\u001b[01;34m__pycache__\u001b[0m/       unetmIoU-0.218.pt  unetmIoU-0.288.pt  unetmIoU-0.735.pt\n",
      "txt.txt            unetmIoU-0.277.pt  unetmIoU-0.289.pt  unetmIoU-0.737.pt\n",
      "unetmIoU-0.204.pt  unetmIoU-0.283.pt  unetmIoU-0.290.pt  unetmIoU-0.739.pt\n",
      "unetmIoU-0.209.pt  unetmIoU-0.284.pt  unetmIoU-0.291.pt  unetmIoU-0.741.pt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "#datasets\n",
    "BATCH_SIZE = 4\n",
    "mode=\"baches\"\n",
    "IMAGE_PATH, MASK_PATH = getpaths(mode)\n",
    "df=create_df(mode)\n",
    "train_set = OperationDataset(IMAGE_PATH, MASK_PATH, df['id'].values)\n",
    "val_set = OperationDataset(IMAGE_PATH, MASK_PATH, df['id'].values)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-21 10:56:08,935]\u001b[0m Using an existing study with name 'optuna4' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d848dcce1984afc9d30b99762b707d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-21 10:56:14,461]\u001b[0m Trial 32 failed with parameters: {} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.93 GiB total capacity; 6.58 GiB already allocated; 4.06 MiB free; 7.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-pcarmona@alges.cl/.local/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_65606/2675833127.py\", line 24, in Objective\n",
      "    history = fit(EPOCHS, model, train_loader, val_loader, criterion, optimizer, sched)\n",
      "  File \"/home/jupyter-pcarmona@alges.cl/Model/Segmentacion/data.py\", line 233, in fit\n",
      "    loss = criterion(output, mask)\n",
      "  File \"/home/jupyter-pcarmona@alges.cl/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/jupyter-pcarmona@alges.cl/.local/lib/python3.9/site-packages/segmentation_models_pytorch/losses/focal.py\", line 88, in forward\n",
      "    loss += self.focal_loss_fn(cls_y_pred, cls_y_true)\n",
      "  File \"/home/jupyter-pcarmona@alges.cl/.local/lib/python3.9/site-packages/segmentation_models_pytorch/losses/_functional.py\", line 70, in focal_loss_with_logits\n",
      "    logpt = F.binary_cross_entropy_with_logits(output, target, reduction=\"none\")\n",
      "  File \"/home/jupyter-pcarmona@alges.cl/.local/lib/python3.9/site-packages/torch/nn/functional.py\", line 3150, in binary_cross_entropy_with_logits\n",
      "    return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.93 GiB total capacity; 6.58 GiB already allocated; 4.06 MiB free; 7.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[33m[W 2023-04-21 10:56:14,462]\u001b[0m Trial 32 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.93 GiB total capacity; 6.58 GiB already allocated; 4.06 MiB free; 7.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m storage_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msqlite:///optuna.db\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Storage in DB.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m,load_if_exists\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, study_name\u001b[39m=\u001b[39mstudy_name, storage\u001b[39m=\u001b[39mstorage_name, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m), pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mMedianPruner(n_startup_trials\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(nt\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m), n_warmup_steps\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(nt\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m)))\n\u001b[0;32m---> 30\u001b[0m study\u001b[39m.\u001b[39;49moptimize(Objective, n_trials\u001b[39m=\u001b[39;49mnt)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mObjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLR, weight_decay\u001b[39m=\u001b[39mWEIGHT_DECAY)\n\u001b[1;32m     21\u001b[0m sched \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mOneCycleLR(optimizer, LR, epochs\u001b[39m=\u001b[39mEPOCHS,\n\u001b[1;32m     22\u001b[0m                                             steps_per_epoch\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader))\n\u001b[0;32m---> 24\u001b[0m history \u001b[39m=\u001b[39m fit(EPOCHS, model, train_loader, val_loader, criterion, optimizer, sched)\n\u001b[1;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m history[\u001b[39m'\u001b[39m\u001b[39mval_miou\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Model/Segmentacion/data.py:233\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39m#forward\u001b[39;00m\n\u001b[1;32m    231\u001b[0m output \u001b[39m=\u001b[39m model(image)\n\u001b[0;32m--> 233\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, mask)\n\u001b[1;32m    234\u001b[0m \u001b[39m#evaluation metrics\u001b[39;00m\n\u001b[1;32m    235\u001b[0m iou_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m mIoU(output, mask)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/segmentation_models_pytorch/losses/focal.py:88\u001b[0m, in \u001b[0;36mFocalLoss.forward\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     85\u001b[0m             cls_y_true \u001b[39m=\u001b[39m cls_y_true[not_ignored]\n\u001b[1;32m     86\u001b[0m             cls_y_pred \u001b[39m=\u001b[39m cls_y_pred[not_ignored]\n\u001b[0;32m---> 88\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfocal_loss_fn(cls_y_pred, cls_y_true)\n\u001b[1;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/segmentation_models_pytorch/losses/_functional.py:70\u001b[0m, in \u001b[0;36mfocal_loss_with_logits\u001b[0;34m(output, target, gamma, alpha, reduction, normalized, reduced_threshold, eps)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute binary focal loss between target and output logits.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mSee :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m    https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/loss/losses.py\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mtype(output\u001b[39m.\u001b[39mtype())\n\u001b[0;32m---> 70\u001b[0m logpt \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(output, target, reduction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnone\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     71\u001b[0m pt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mlogpt)\n\u001b[1;32m     73\u001b[0m \u001b[39m# compute the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:3150\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m   3148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[0;32m-> 3150\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.93 GiB total capacity; 6.58 GiB already allocated; 4.06 MiB free; 7.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import segmentation_models_pytorch as smp\n",
    "def Objective(trial):\n",
    "    ENCODER_NAME = 'resnet18'#'timm-regnetx_002'#trial.suggest_categorical('encoder',['resnet50','resnet18','timm-efficientnet-b1'])#'mobilenet_v2'  # 'mobilenet_v2', 'resnet50', 'resnet34'\n",
    "    ENCODER_WEIGHTS = 'imagenet'  # None, 'imagenet', 'ssl', 'swsl'\n",
    "    EPOCHS = 10\n",
    "    ENCODER_DEPTH = 3\n",
    "    POOLING = 'avg'  #  'avg', 'max'\n",
    "    DROPOUT = 0.1\n",
    "    CHANS=3\n",
    "\n",
    "    model=getModel('unet++')#trial.suggest_categorical('model',['unet++','fpn','linknet']))\n",
    "    LR = 0.002#trial.suggest_float('lr', 1e-5, 1e-2)\n",
    "    WEIGHT_DECAY = 0.004#trial.suggest_float('weight_decay', 1e-5, 1e-2)\n",
    "    #criterion = trial.suggest_categorical('criterion', ['LV','DL','Focal','Tversky'])\n",
    "    criterion = getCriterion('Focal')#trial.suggest_categorical('criterion', ['LV','DL','Focal','Tversky']))\n",
    "    \n",
    "    # optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=LR),])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, LR, epochs=EPOCHS,\n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "\n",
    "    history = fit(EPOCHS, model, train_loader, val_loader, criterion, optimizer, sched)\n",
    "    return history['val_miou'][-1]\n",
    "nt=60\n",
    "study_name = 'optuna4'  # Unique identifier of the study.\n",
    "storage_name = 'sqlite:///optuna.db'  # Storage in DB.\n",
    "study = optuna.create_study(direction='maximize',load_if_exists=True, study_name=study_name, storage=storage_name, sampler=optuna.samplers.TPESampler(seed=42), pruner=optuna.pruners.MedianPruner(n_startup_trials=int(nt/3), n_warmup_steps=int(nt/3)))\n",
    "study.optimize(Objective, n_trials=nt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x=OperationDataset(getpaths('baches')[0],getpaths('baches')[1],df['id'].values)\n",
    "model=\"unetmIoU-0.741.pt\"\n",
    "model=torch.load(model)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c01b196cf946dfae2a88a012e5b0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=162, description='id', max=325), Output()), _dom_classes=('widget-interaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.pred(id)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "import torch\n",
    "def load(id):\n",
    "    img=np.load(getpaths(mode)[0]+str(id)+'.npz')['arr_0'][:,:,:3]\n",
    "    mask=np.load(getpaths(mode)[1]+str(id)+'.npz')['arr_0']\n",
    "    return img,mask\n",
    "def plot(id,pred=None):\n",
    "    img,mask=load(id)\n",
    "    if pred is not None:\n",
    "        fig, ax =  plt.subplots(1, 3, figsize=(18,18))\n",
    "    else:\n",
    "        fig, ax =  plt.subplots(1, 2, figsize=(18,18))\n",
    "    #set fixed colors of mask and pred for 7 classes\n",
    "    colors = np.array([[0, 0, 0], [0, 0, 255], [0, 255, 0], [255, 0, 0], [255, 255, 0], [255, 0, 255], [0, 255, 255]])\n",
    "    mask = colors[mask]\n",
    "    if pred is not None:\n",
    "        pred = colors[pred]\n",
    "    ax[0].imshow(img)\n",
    "    ax[1].imshow(mask)\n",
    "    if pred is not None:\n",
    "        ax[2].imshow(pred)\n",
    "    plt.show()\n",
    "def pred(id):\n",
    "    img,mask=load(id)\n",
    "    img=torch.from_numpy(img).permute(2,0,1).unsqueeze(0).float()\n",
    "    pred=model(img.to(device))\n",
    "    pred=torch.argmax(pred, dim=1).cpu().squeeze().detach().numpy()\n",
    "    plot(id,pred)\n",
    "interact(pred, id=(0,len(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
